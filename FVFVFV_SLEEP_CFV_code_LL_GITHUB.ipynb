{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030dd00c",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23903e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import levene, shapiro, f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "from tableone import TableOne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fcfb1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1908ed",
   "metadata": {},
   "source": [
    "## Functions required to run code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5cca3",
   "metadata": {},
   "source": [
    "#### Function to fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(series):\n",
    "    if series.notna().all():\n",
    "        return series\n",
    "    else:\n",
    "        non_na_values = series.dropna()\n",
    "        if non_na_values.size > 0:\n",
    "            fill_value = non_na_values.iloc[0]\n",
    "            return np.where(series.isna(), np.full_like(series, fill_value), series)\n",
    "        else:\n",
    "            return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e580c",
   "metadata": {},
   "source": [
    "#### Function to convert dataframe to sequences for HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM_Sequences(df):\n",
    "\n",
    "    selected_columns = df.columns[6:].tolist()\n",
    "    \n",
    "    # List of participant ids that will be iterated over\n",
    "    p_ids = df['p_id'].unique()\n",
    "\n",
    "    # List to store individual selected_data arrays along with participant IDs\n",
    "    selected_data_list = []\n",
    "\n",
    "    # List to store sequence lengths\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for p_id in p_ids:\n",
    "        # Step 1: Create a boolean Series\n",
    "        bool_loc = df['p_id'] == p_id\n",
    "    \n",
    "        # Step 2: Get indices where the condition is True\n",
    "        row_index = bool_loc[bool_loc].index.values  \n",
    "        \n",
    "        # Step 3: Sort DataFrame by the numeric version of the sorting column in ascending order \n",
    "        df_sorted = df.loc[row_index, selected_columns + ['timepoint']].sort_values(by='timepoint', ascending=True)\n",
    "        \n",
    "        # Step 4: Extract data from selected columns\n",
    "        selected_data = df_sorted[selected_columns].values.round(3)\n",
    "\n",
    "        # Append selected_data to the list\n",
    "        selected_data_list.append(selected_data)\n",
    "        \n",
    "        # Append the length of the selected_data to the sequence_lengths list\n",
    "        sequence_lengths.append(len(selected_data))\n",
    "        \n",
    "    # Concatenate the sequences into a single array\n",
    "    final_concatenated_data = np.concatenate(selected_data_list)\n",
    "\n",
    "    # Array of sequence lengths\n",
    "    sequence_lengths_array = np.array(sequence_lengths)\n",
    "    \n",
    "    return final_concatenated_data, sequence_lengths_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_with_seed(data_dict, n_clusters, cluster_method = 'GMM'):\n",
    "    \n",
    "    # Get data from dictionary\n",
    "    if cluster_method == 'GMM':\n",
    "        train_data = data_dict.get(\"train_data\")\n",
    "    \n",
    "    elif cluster_method == 'HMM':  \n",
    "        train_sequences = data_dict.get(\"train_sequences\")\n",
    "        train_sequence_lengths = data_dict.get(\"train_sequence_lengths\")\n",
    "        \n",
    "    ll = []  # To store log-likelihood for each seed\n",
    "    seeds = np.arange(1, 100)  # Seeds from 1 to 99 (inclusive)\n",
    "    \n",
    "    # Loop over each seed\n",
    "    for seed in seeds:\n",
    "        \n",
    "        if cluster_method == 'GMM':\n",
    "            # Initialize the model\n",
    "            gmm = GaussianMixture(n_components=n_clusters, init_params='k-means++', covariance_type='diag', random_state=seed)\n",
    "            # Fit the model\n",
    "            gmm.fit(train_data)\n",
    "            # Calculate log-likelihood (LL) for the current seed\n",
    "            logl = gmm.score(train_data)\n",
    "             \n",
    "        elif cluster_method == 'HMM':  \n",
    "            # Initialize the model\n",
    "            model = GaussianHMM(n_components=n_clusters, covariance_type=\"diag\", random_state = seed) ## ADDED\n",
    "            # Fit the model\n",
    "            model.fit(train_sequences, train_sequence_lengths)  # Train with lengths\n",
    "            # Calculate log-likelihood (LL) for the current seed\n",
    "            logl = model.score(train_sequences, train_sequence_lengths)\n",
    "           \n",
    "        # append score to list   \n",
    "        ll.append(logl)\n",
    "            \n",
    "    # Find the median log-likelihood\n",
    "    median_ll = np.median(ll)\n",
    "        \n",
    "    # Find the seed associated with the median LL\n",
    "    median_seed = seeds[np.argmin(np.abs(ll - median_ll))]  # Seed with LL closest to the median LL\n",
    "        \n",
    "    # Refit the model with the best (median) seed\n",
    "    if cluster_method == 'GMM':\n",
    "        # re-initialize the model\n",
    "        median_model = GaussianMixture(n_components=n_clusters, init_params='k-means++', covariance_type='diag', random_state=median_seed)\n",
    "        # Re-fit the model\n",
    "        median_model.fit(train_data)  # Fit again with the median seed\n",
    "        \n",
    "    elif cluster_method == 'HMM':  \n",
    "        # re-initialize the model\n",
    "        median_model = GaussianHMM(n_components=n_clusters, covariance_type=\"diag\", random_state = median_seed) ## ADDED\n",
    "        # Re-fit the model\n",
    "        median_model.fit(train_sequences, train_sequence_lengths)  # Train with lengths\n",
    "        \n",
    "    return median_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15553e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a991b4",
   "metadata": {},
   "source": [
    "## Data Preperation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79ceb1",
   "metadata": {},
   "source": [
    "#### Read in data and organise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"radar_ids_features_sd.csv\")\n",
    "df = df.rename(columns={'record_name': 'timepoint'})\n",
    "df_predictors = pd.read_csv(\"df_predictors.csv\")\n",
    "\n",
    "# Apply the function to the 'age' column grouped by 'p_id'\n",
    "df['age_all'] = df.groupby('p_id')['age'].transform(fill)\n",
    "df['age_all'] = df['age_all'] / 10\n",
    "\n",
    "# Apply the function to the 'gender' column grouped by 'p_id'\n",
    "df['gender_all'] = df.groupby('p_id')['gender'].transform(fill)\n",
    "\n",
    "# Data selection & cleaning\n",
    "total_data = df[['p_id', 'timepoint', 'sleep_day','IDS_TOTAL', 'age_all', 'gender_all',\n",
    "       'total_sleep_time_mean', 'total_sleep_time_sd',\n",
    "        'awake_pct_mean' ,\n",
    "        'sleep_onset_mean',\n",
    "       'sleep_onset_sd', 'sleep_offset_mean', 'sleep_offset_sd',\n",
    "       'sleep_efficiency_mean', 'sleep_efficiency_sd'  ]]\n",
    "\n",
    "# Drop rows with sleep_available >= 3\n",
    "total_data = total_data[total_data['sleep_day'] >= 3]\n",
    "\n",
    "#Record ID rename\n",
    "replacements = {\n",
    "    \"12_month_assessmen_arm_1\": 12,\n",
    "    \"15_month_assessmen_arm_1\": 15,\n",
    "    \"18_month_assessmen_arm_1\": 18,\n",
    "    \"21_month_assessmen_arm_1\": 21,\n",
    "    \"24_month_assessmen_arm_1\": 24,\n",
    "    \"27_month_assessmen_arm_1\": 27,\n",
    "    \"30_month_assessmen_arm_1\": 30,\n",
    "    \"33_month_assessmen_arm_1\": 33,\n",
    "    \"36_month_assessmen_arm_1\": 36,\n",
    "    \"39_month_assessmen_arm_1\": 39,\n",
    "    \"3_month_assessment_arm_1\": 3,\n",
    "    \"6_month_assessment_arm_1\": 6,\n",
    "    \"9_month_assessment_arm_1\": 9,\n",
    "    \"enrolment_arm_1\": 0\n",
    "}\n",
    "\n",
    "# Use map function\n",
    "total_data.timepoint = total_data.timepoint.map(replacements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de95cb",
   "metadata": {},
   "source": [
    "#### Impute missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f517a4-8ca0-4227-b33a-be1566bdb277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "missing_values_df = total_data.isna().sum().reset_index()\n",
    "non_missing_values_df = total_data.notna().sum().reset_index(drop=True)\n",
    "na_df = missing_values_df\n",
    "na_df.columns = ['Variable', 'Missing Values']\n",
    "na_df['Non-missing Values'] = non_missing_values_df\n",
    "print(na_df)\n",
    "\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8578c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = total_data.copy()\n",
    "\n",
    "# Separate the participant ID and other columns without missing data\n",
    "non_numeric_cols = total_data[['p_id', 'gender_all']]  # Assuming 'p_id' is the participant ID\n",
    "numeric_cols = total_data.drop(columns=['p_id', 'gender_all'])  # Exclude the non-numeric column(s)\n",
    "\n",
    "# Impute only the numeric columns\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_numeric_data = imputer.fit_transform(numeric_cols)\n",
    "imputed_numeric_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols.columns)\n",
    "imputed_df = pd.concat([non_numeric_cols.reset_index(drop=True), imputed_numeric_df], axis=1)\n",
    "\n",
    "total_data = imputed_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612617d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7aa8a7",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e876e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Range of states to evaluate\n",
    "cluster_options =  [2, 3, 4, 5, 6, 7, 8, 9 ,10]\n",
    "\n",
    "#Setting up stratified CFV\n",
    "n_folds = 5\n",
    "group_id = pd.factorize(total_data['p_id'])[0]\n",
    "GFK = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "fold_test_scores_GMM = np.zeros([n_folds, len(cluster_options)])\n",
    "fold_test_scores_HMM = np.zeros([n_folds, len(cluster_options)])\n",
    "\n",
    "# Loop over each choice of number of states\n",
    "cluster_counter = 0\n",
    "for n_clusters in cluster_options:\n",
    "    \n",
    "    fold_counter = 0\n",
    "    print('Testing number of cluster: ' + str(n_clusters))\n",
    "    \n",
    "    # Do repeated cross-fold validation\n",
    "    for train_index, test_index in GFK.split(total_data, groups = group_id):\n",
    "        \n",
    "        # define data splits \n",
    "        train_data = total_data.iloc[train_index,:]\n",
    "        test_data = total_data.iloc[test_index,:]\n",
    "        \n",
    "        # transform the data\n",
    "        train_data.iloc[:,6:] = scaler.fit_transform(train_data.iloc[:,6:])\n",
    "        test_data.iloc[:,6:] = scaler.transform(test_data.iloc[:,6:])\n",
    "        \n",
    "        # Choose GMM\n",
    "        cluster_method = 'GMM'\n",
    "        \n",
    "        # organise training data, put in dictionary to pass to seed function \n",
    "        selected_columns = total_data.columns[6:].tolist()\n",
    "        train_data_GMM = train_data[selected_columns]\n",
    "        data_dict = {\"train_data\": train_data_GMM}\n",
    "        \n",
    "        # find median_fit model \n",
    "        median_model_GMM = clustering_with_seed(data_dict, n_clusters, cluster_method)\n",
    "        \n",
    "        # organise test data, get test set LL on  median_fit model\n",
    "        test_data_GMM = test_data[selected_columns]\n",
    "        fold_test_scores_GMM[fold_counter,cluster_counter] = median_model_GMM.score(test_data_GMM)\n",
    "        \n",
    "        # Choose HMM\n",
    "        cluster_method = 'HMM'\n",
    "        \n",
    "        # organise training sequences, put in dictionary to pass to seed function \n",
    "        train_sequences, train_sequence_lengths = HMM_Sequences(train_data)\n",
    "        data_dict = {\"train_sequences\": train_sequences,\n",
    "                     \"train_sequence_lengths\": train_sequence_lengths}\n",
    "        \n",
    "        # find median_fit model \n",
    "        median_model_HMM = clustering_with_seed(data_dict, n_clusters, cluster_method)\n",
    "        \n",
    "        # organise test sequences, get test set LL on  median_fit model\n",
    "        test_sequnces, test_sequence_lengths = HMM_Sequences(test_data)\n",
    "        fold_test_scores_HMM[fold_counter,cluster_counter] = median_model_HMM.score(test_sequnces, test_sequence_lengths)\n",
    "        \n",
    "        fold_counter +=1\n",
    "        \n",
    "    cluster_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83013ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True)\n",
    "fig.suptitle(\"Sleep - NoIter - Average Log-Likelihood vs. Number of Cluster\")\n",
    "\n",
    "CFV_scores_GMM = np.mean(fold_test_scores_GMM,0) \n",
    "axs[0].plot(cluster_options, CFV_scores_GMM, marker = 'o', color='r', label='GMM')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_ylabel(\"Average Log-Likelihood\")\n",
    "axs[0].set_title(\"GMM\")\n",
    "\n",
    "CFV_scores_HMM = np.mean(fold_test_scores_HMM,0)\n",
    "axs[1].plot(cluster_options, CFV_scores_HMM, marker = 'o', color='g', label='HMM')\n",
    "axs[1].grid(True)\n",
    "axs[1].set_xticks(range(min(cluster_options),len(cluster_options)+min(cluster_options))) \n",
    "axs[1].set_xticklabels(cluster_options)\n",
    "axs[1].set_xlabel(\"Number of clusters\")\n",
    "axs[1].set_ylabel(\"Average Log-Likelihood\")\n",
    "axs[1].set_title(\"HMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790432a-b9cb-4d0f-82c4-fc878c007587",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CFV_scores_GMM)\n",
    "print(CFV_scores_HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb1921-62c0-4cf1-bf61-65aa7b7fb176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the data\n",
    "sleep_selection = {\n",
    "    \"Clusters\": cluster_options,\n",
    "    \"SLEEP_LL_GMM\": CFV_scores_GMM,\n",
    "    \"SLEEP_LL_HMM\": CFV_scores_HMM\n",
    "}\n",
    "# Create a DataFrame from the dictionary\n",
    "plotdata = pd.DataFrame(sleep_selection)\n",
    "plotdata.to_csv(\"xxxx.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c879dfb",
   "metadata": {},
   "source": [
    "### Final Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6861c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "total_data_scaled = total_data.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "total_data_scaled.iloc[:,6:] = scaler.fit_transform(total_data.iloc[:,6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2378d9-ccd7-4d1c-aeb3-15ddbe3e2ba4",
   "metadata": {},
   "source": [
    "#### GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969690f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose GMM and number of clusters \n",
    "cluster_method = 'GMM'\n",
    "optimal_num_clusters = 4\n",
    "        \n",
    "# Organise data, put in dictionary to pass to seed function \n",
    "selected_columns = total_data_scaled.iloc[:, 6:15].columns.tolist() ##### CHANGED BY ME\n",
    "total_data_scaled_GMM = total_data_scaled[selected_columns]\n",
    "data_dict = {\"train_data\": total_data_scaled_GMM}\n",
    "\n",
    "# find median_fit model \n",
    "final_model_GMM = clustering_with_seed(data_dict, optimal_num_clusters, cluster_method)\n",
    "\n",
    "# Predict cluster labels for the scaled dataset\n",
    "final_labels = final_model_GMM.predict(total_data_scaled_GMM)\n",
    "\n",
    "# Add the predicted labels as a new column in your dataframe\n",
    "total_data_scaled['gmm'] = final_labels\n",
    "\n",
    "# Calculate the number of unique participants\n",
    "num_unique_participants = total_data_scaled['p_id'].nunique()\n",
    "print(\"Number of unique participants:\", num_unique_participants)\n",
    "\n",
    "# Group by the predicted labels and calculate mean statistics\n",
    "state_profiles = total_data_scaled.groupby('gmm').agg({\n",
    "    'total_sleep_time_mean': ['mean'],\n",
    "    'sleep_onset_mean': ['mean'],\n",
    "    'sleep_offset_mean': ['mean'],\n",
    "    'sleep_efficiency_mean': ['mean'],\n",
    "    'awake_pct_mean': ['mean'],\n",
    "    'total_sleep_time_sd': ['mean'],\n",
    "    'sleep_efficiency_sd': ['mean'],\n",
    "    'sleep_onset_sd': ['mean'],    \n",
    "    'sleep_offset_sd': ['mean'],\n",
    "}).reset_index()\n",
    "\n",
    "# Add a column for the number of sequences in each cluster\n",
    "state_profiles['Num_Sequences'] = total_data_scaled.groupby('gmm').size().values\n",
    "display(state_profiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8605ee-a480-4c5a-8c42-1a2862afffd5",
   "metadata": {},
   "source": [
    "#### HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd454df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose HMM\n",
    "cluster_method = 'HMM'\n",
    "optimal_num_clusters = 4\n",
    "\n",
    "total_sequences, total_sequence_lengths = HMM_Sequences(total_data_scaled.iloc[:, 0:15])\n",
    "data_dict = {\"train_sequences\": total_sequences,\n",
    "             \"train_sequence_lengths\": total_sequence_lengths}\n",
    "\n",
    "# find median_fit model \n",
    "final_model_HMM = clustering_with_seed(data_dict, optimal_num_clusters, cluster_method)\n",
    "\n",
    "# Predict the most likely state sequence for the entire dataset\n",
    "predicted_labels = final_model_HMM.predict(total_sequences, total_sequence_lengths)\n",
    "\n",
    "# Add predicted labels to your dataframe (if you want to attach them to the original data)\n",
    "total_data_scaled['hmm'] = predicted_labels\n",
    "\n",
    "# Calculate the number of unique participants\n",
    "num_unique_participants = total_data_scaled['p_id'].nunique()\n",
    "print(\"Number of unique participants:\", num_unique_participants)\n",
    "\n",
    "# Group by the predicted labels and calculate mean statistics\n",
    "state_profiles = total_data_scaled.groupby('hmm').agg({\n",
    "    'total_sleep_time_mean': ['mean'],\n",
    "    'sleep_onset_mean': ['mean'],\n",
    "    'sleep_offset_mean': ['mean'],\n",
    "    'sleep_efficiency_mean': ['mean'],\n",
    "    'awake_pct_mean': ['mean'],\n",
    "    'total_sleep_time_sd': ['mean'],\n",
    "    'sleep_efficiency_sd': ['mean'],\n",
    "    'sleep_onset_sd': ['mean'],    \n",
    "    'sleep_offset_sd': ['mean'],\n",
    "}).reset_index()\n",
    "\n",
    "# Add a column for the number of sequences in each cluster\n",
    "state_profiles['Num_Sequences'] = total_data_scaled.groupby('hmm').size().values\n",
    "display(state_profiles)\n",
    "\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d65e8-9cdf-4a6b-964c-d061d6eca880",
   "metadata": {},
   "source": [
    "#### HMM Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c223dda-a4f6-4335-a9fc-4d7127a9fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the transition matrix\n",
    "transition_probabilities = final_model_HMM.transmat_\n",
    "\n",
    "# Round the transition probabilities to 2 decimals\n",
    "rounded_transition_probabilities = np.round(transition_probabilities, 3)\n",
    "print(\"Rounded Transition Probabilities:\")\n",
    "print(rounded_transition_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa3237-6f4a-4764-b378-2a84fb33b5a4",
   "metadata": {},
   "source": [
    "#### HMM state Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bb7f1-c023-4c9d-94dd-c8cd772b20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE DATA FOR PREDICTION ANALYSIS \n",
    "# Get the posterior probabilities for each state at each timepoint\n",
    "state_probabilities = final_model_HMM.predict_proba(total_sequences, total_sequence_lengths)\n",
    "\n",
    "# Create a DataFrame to store the state probabilities\n",
    "state_prob_df = pd.DataFrame(state_probabilities, columns=[f'State_Prob_{i}' for i in range(optimal_num_clusters)])\n",
    "state_prob_df['timepoint'] = total_data_scaled['timepoint'].values\n",
    "state_prob_df['p_id'] = total_data_scaled['p_id'].values\n",
    "display(state_prob_df)\n",
    "\n",
    "total_data_scaled_probs = pd.merge(state_prob_df, total_data_scaled, on=['p_id', 'timepoint'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a288341-ebe7-47ce-9a6e-920273324b3c",
   "metadata": {},
   "source": [
    "### Relabelling/Reordering the classes for GMM & HMM for Descriptives & prediction work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486402e9-a013-4cc1-b654-703d63e4b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELABELING THE CLASSES: \n",
    "# Define the mapping (old value -> new value)\n",
    "hmm_mapping = {0: 2, \n",
    "               1: 1, \n",
    "               2: 4,\n",
    "               3: 3} \n",
    "gmm_mapping = {0: 4, \n",
    "               1: 3, \n",
    "               2: 1, \n",
    "               3: 2} \n",
    "\n",
    "# Apply the mapping to the 'Category' column\n",
    "total_data_scaled['hmm'] = total_data_scaled['hmm'].replace(hmm_mapping)\n",
    "total_data_scaled['gmm'] = total_data_scaled['gmm'].replace(gmm_mapping)\n",
    "\n",
    "# For the probs df\n",
    "total_data_scaled_probs['hmm'] = total_data_scaled_probs['hmm'].replace(hmm_mapping)\n",
    "total_data_scaled_probs['gmm'] = total_data_scaled_probs['gmm'].replace(gmm_mapping)\n",
    "\n",
    "# Rename columns using a dictionary\n",
    "total_data_scaled_probs.rename(columns={'State_Prob_0': 'State_Prob_2', \n",
    "                                        'State_Prob_1': 'State_Prob_1', \n",
    "                                        'State_Prob_2': 'State_Prob_4', \n",
    "                                        'State_Prob_3': 'State_Prob_3'}, inplace=True)\n",
    "\n",
    "# Print the DataFrame after renaming columns\n",
    "print(\"\\nDataFrame after renaming columns:\")\n",
    "print(total_data_scaled_probs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b01e4-9727-434f-ac60-cf1e01d54e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency of values in the 'Category' column\n",
    "frequency = total_data_scaled_probs['hmm'].value_counts()\n",
    "print(frequency)\n",
    "\n",
    "frequency_2 = total_data_scaled['hmm'].value_counts()\n",
    "print(frequency_2)\n",
    "\n",
    "frequency_gmm = total_data_scaled['gmm'].value_counts()\n",
    "print(frequency_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea6cd1-9fbf-4946-8496-881622ec1ecf",
   "metadata": {},
   "source": [
    "### Demographic & Clinical Differences of the groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016614d-4a98-43c5-88e8-bc1f4d4f936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_scaled_copy = total_data_scaled.copy()\n",
    "\n",
    "merged_selected = total_data_scaled_copy[columns_to_merge]\n",
    "total_data_withcara = pd.merge(total_data_scaled_copy, df_predictors, on='p_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081ad8e-1635-44c0-8d29-227bca399fcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the columns to include in the TableOne summary\n",
    "columns = ['age', 'gender_all_x', 'IDS_TOTAL_x',# 'IDS_CAT',\n",
    "           'recruitmentsite', 'mh_family_TOTAL', 'Mental_comorbidity',\n",
    "       'Physical_comorbidity','WSAS_TOTAL', 'GAD7_TOTAL', 'EMPLOYMENT_STATUS'] #, 'ETHCAT2', \n",
    "\n",
    "# Create the TableOne grouped by HMM\n",
    "table_activity_states = TableOne(total_data_withcara,                                       \n",
    "                                      columns=columns, \n",
    "                                      categorical=['gender_all_x', 'recruitmentsite', 'mh_family_TOTAL',\n",
    "                                                  'Mental_comorbidity',\n",
    "                                                   'Physical_comorbidity', 'EMPLOYMENT_STATUS'], \n",
    "                                      groupby='hmm',  \n",
    "                                     # pval=True\n",
    "                                )\n",
    "                                      #normal_test=True)  \n",
    "# Display the grouped TableOne\n",
    "print(table_activity_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
