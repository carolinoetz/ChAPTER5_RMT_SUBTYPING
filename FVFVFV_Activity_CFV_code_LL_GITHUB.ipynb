{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030dd00c",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23903e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021fcfb1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1908ed",
   "metadata": {},
   "source": [
    "## Functions required to run code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5cca3",
   "metadata": {},
   "source": [
    "#### Function to fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(series):\n",
    "    if series.notna().all():\n",
    "        return series\n",
    "    else:\n",
    "        non_na_values = series.dropna()\n",
    "        if non_na_values.size > 0:\n",
    "            fill_value = non_na_values.iloc[0]\n",
    "            return np.where(series.isna(), np.full_like(series, fill_value), series)\n",
    "        else:\n",
    "            return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e580c",
   "metadata": {},
   "source": [
    "#### Function to convert dataframe to sequences for HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HMM_Sequences(df):\n",
    "\n",
    "    selected_columns = df.columns[6:].tolist()\n",
    "    \n",
    "    # List of participant ids that will be iterated over --> copied from code above\n",
    "    p_ids = df['p_id'].unique()\n",
    "\n",
    "    # List to store individual selected_data arrays along with participant IDs\n",
    "    selected_data_list = []\n",
    "\n",
    "    # List to store sequence lengths\n",
    "    sequence_lengths = []\n",
    "\n",
    "    for p_id in p_ids:\n",
    "        # Step 1: Create a boolean Series\n",
    "        bool_loc = df['p_id'] == p_id\n",
    "    \n",
    "        # Step 2: Get indices where the condition is True\n",
    "        row_index = bool_loc[bool_loc].index.values  \n",
    "        \n",
    "        # Step 3: Sort DataFrame by the numeric version of the sorting column in ascending order \n",
    "        df_sorted = df.loc[row_index, selected_columns + ['timepoint']].sort_values(by='timepoint', ascending=True)\n",
    "        \n",
    "        # Step 4: Extract data from selected columns\n",
    "        selected_data = df_sorted[selected_columns].values.round(3)\n",
    "\n",
    "        # Append selected_data to the list\n",
    "        selected_data_list.append(selected_data)\n",
    "        \n",
    "        # Append the length of the selected_data to the sequence_lengths list\n",
    "        sequence_lengths.append(len(selected_data))\n",
    "        \n",
    "    # Concatenate the sequences into a single array\n",
    "    final_concatenated_data = np.concatenate(selected_data_list)\n",
    "\n",
    "    # Array of sequence lengths\n",
    "    sequence_lengths_array = np.array(sequence_lengths)\n",
    "    \n",
    "    return final_concatenated_data, sequence_lengths_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78197d6-5e60-4519-bffa-902a1c117fa2",
   "metadata": {},
   "source": [
    "#### Functioning for GMM/HMM with seed optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_with_seed(data_dict, n_clusters, cluster_method = 'GMM'):\n",
    "    \n",
    "    # Get data from dictionary\n",
    "    if cluster_method == 'GMM':\n",
    "        train_data = data_dict.get(\"train_data\")\n",
    "    \n",
    "    elif cluster_method == 'HMM':  \n",
    "        train_sequences = data_dict.get(\"train_sequences\")\n",
    "        train_sequence_lengths = data_dict.get(\"train_sequence_lengths\")\n",
    "        \n",
    "    ll = []  # To store log-likelihood for each seed\n",
    "    seeds = np.arange(1, 100)  # Seeds from 1 to 99 (inclusive)\n",
    "    \n",
    "    # Loop over each seed\n",
    "    for seed in seeds:\n",
    "        \n",
    "        if cluster_method == 'GMM':\n",
    "            # Initialize the model\n",
    "            gmm = GaussianMixture(n_components=n_clusters, init_params='k-means++', covariance_type='diag', random_state=seed)\n",
    "            # Fit the model\n",
    "            gmm.fit(train_data)\n",
    "            # Calculate log-likelihood (LL) for the current seed\n",
    "            logl = gmm.score(train_data)\n",
    "             \n",
    "        elif cluster_method == 'HMM':  \n",
    "            # Initialize the model\n",
    "            model = GaussianHMM(n_components=n_clusters, covariance_type=\"diag\", random_state = seed) ## ADDED\n",
    "            # Fit the model\n",
    "            model.fit(train_sequences, train_sequence_lengths)  # Train with lengths\n",
    "            # Calculate log-likelihood (LL) for the current seed\n",
    "            logl = model.score(train_sequences, train_sequence_lengths)\n",
    "           \n",
    "        # append score to list   \n",
    "        ll.append(logl)\n",
    "            \n",
    "    # Find the median log-likelihood\n",
    "    median_ll = np.median(ll)\n",
    "        \n",
    "    # Find the seed associated with the median LL\n",
    "    median_seed = seeds[np.argmin(np.abs(ll - median_ll))]  # Seed with LL closest to the median LL\n",
    "        \n",
    "    # Refit the model with the best (median) seed\n",
    "    if cluster_method == 'GMM':\n",
    "        # re-initialize the model\n",
    "        median_model = GaussianMixture(n_components=n_clusters, init_params='k-means++', covariance_type='diag', random_state=median_seed)\n",
    "        # Re-fit the model\n",
    "        median_model.fit(train_data)  # Fit again with the median seed\n",
    "        \n",
    "    elif cluster_method == 'HMM':  \n",
    "        # re-initialize the model\n",
    "        median_model = GaussianHMM(n_components=n_clusters, covariance_type=\"diag\", random_state = median_seed) ## ADDED\n",
    "        # Re-fit the model\n",
    "        median_model.fit(train_sequences, train_sequence_lengths)  # Train with lengths\n",
    "        \n",
    "    return median_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15553e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a991b4",
   "metadata": {},
   "source": [
    "## Data Preperation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79ceb1",
   "metadata": {},
   "source": [
    "#### Read in data and organise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"radar_ids_features_hr_activity_steps.csv\")\n",
    "df = df.rename(columns={'record_name': 'timepoint'})\n",
    "\n",
    "# Apply the function to the 'age' column grouped by 'p_id'\n",
    "df['age_all'] = df.groupby('p_id')['age'].transform(fill)\n",
    "df['age_all'] = df['age_all'] / 10\n",
    "\n",
    "# Apply the function to the 'gender' column grouped by 'p_id'\n",
    "df['gender_all'] = df.groupby('p_id')['gender'].transform(fill)\n",
    "\n",
    "# Data selection & cleaning\n",
    "total_data = df[['p_id', 'timepoint', 'act_day', 'IDS_TOTAL', 'age_all', 'gender_all',\n",
    "                 'Sedentary_time_mean', 'Light_activity_mean', 'Moderate_activity_mean',\n",
    "                 'Vigorous_activity_mean', 'Nighttime_activity_mean',\n",
    "                 'Total_Daily_Calories_mean',\n",
    "                 'Sedentary_time_sd', 'Light_activity_sd',\n",
    "                 'Moderate_activity_sd', 'Vigorous_activity_sd', 'Nighttime_activity_sd',\n",
    "                 'Total_Daily_Calories_sd'\n",
    "                 ]]\n",
    "\n",
    "# Drop rows with act_available >= 3\n",
    "total_data = total_data[total_data['act_day'] >= 3]\n",
    "\n",
    "#Record ID rename\n",
    "replacements = {\n",
    "    \"12_month_assessmen_arm_1\": 12,\n",
    "    \"15_month_assessmen_arm_1\": 15,\n",
    "    \"18_month_assessmen_arm_1\": 18,\n",
    "    \"21_month_assessmen_arm_1\": 21,\n",
    "    \"24_month_assessmen_arm_1\": 24,\n",
    "    \"27_month_assessmen_arm_1\": 27,\n",
    "    \"30_month_assessmen_arm_1\": 30,\n",
    "    \"33_month_assessmen_arm_1\": 33,\n",
    "    \"36_month_assessmen_arm_1\": 36,\n",
    "    \"39_month_assessmen_arm_1\": 39,\n",
    "    \"3_month_assessment_arm_1\": 3,\n",
    "    \"6_month_assessment_arm_1\": 6,\n",
    "    \"9_month_assessment_arm_1\": 9,\n",
    "    \"enrolment_arm_1\": 0\n",
    "}\n",
    "\n",
    "# Use map function\n",
    "total_data.timepoint = total_data.timepoint.map(replacements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de95cb",
   "metadata": {},
   "source": [
    "#### Visualise missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b7bb1-1cac-46d0-8946-a50e2b61c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "missing_values_df = total_data.isna().sum().reset_index()\n",
    "non_missing_values_df = total_data.notna().sum().reset_index(drop=True)\n",
    "\n",
    "# Combine both into a single DataFrame\n",
    "na_df = missing_values_df\n",
    "na_df.columns = ['Variable', 'Missing Values']\n",
    "na_df['Non-missing Values'] = non_missing_values_df\n",
    "print(na_df)\n",
    "\n",
    "# Reset the setting to default after printing\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71fb3e-d229-49f5-b67e-ddb4e029786a",
   "metadata": {},
   "source": [
    "#### Impute missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8578c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = total_data.copy()\n",
    "\n",
    "# Separate the participant ID and other columns without missing data\n",
    "non_numeric_cols = total_data[['p_id', 'gender_all']]  \n",
    "numeric_cols = total_data.drop(columns=['p_id', 'gender_all'])  \n",
    "\n",
    "# Impute \n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "imputed_numeric_data = imputer.fit_transform(numeric_cols)\n",
    "\n",
    "# Convert the imputed numeric data back to a DataFrame\n",
    "imputed_numeric_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols.columns)\n",
    "imputed_df = pd.concat([non_numeric_cols.reset_index(drop=True), imputed_numeric_df], axis=1)\n",
    "\n",
    "total_data = imputed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc3921-8121-4ba6-9792-c2b7e0676e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOUBLE CHECK ALL REMOVED\n",
    "pd.set_option('display.max_rows', None)\n",
    "missing_values_df = total_data.isna().sum().reset_index()\n",
    "non_missing_values_df = total_data.notna().sum().reset_index(drop=True)\n",
    "na_df = missing_values_df\n",
    "na_df.columns = ['Variable', 'Missing Values']\n",
    "na_df['Non-missing Values'] = non_missing_values_df\n",
    "print(na_df)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612617d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7aa8a7",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e876e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Scalers\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Range of states to evaluate\n",
    "cluster_options =  [2, 3, 4, 5, 6, 7, 8, 9 ,10]\n",
    "\n",
    "#Setting up stratified CFV\n",
    "n_folds = 5\n",
    "group_id = pd.factorize(total_data['p_id'])[0]\n",
    "GFK = GroupKFold(n_splits=n_folds)\n",
    "\n",
    "fold_test_scores_GMM = np.zeros([n_folds, len(cluster_options)])\n",
    "fold_test_scores_HMM = np.zeros([n_folds, len(cluster_options)])\n",
    "\n",
    "# Loop over each choice of number of states\n",
    "cluster_counter = 0\n",
    "for n_clusters in cluster_options:\n",
    "    \n",
    "    fold_counter = 0\n",
    "    print('Testing number of cluster: ' + str(n_clusters))\n",
    "    \n",
    "    # Do repeated cross-fold validation\n",
    "    for train_index, test_index in GFK.split(total_data, groups = group_id):\n",
    "        \n",
    "        # define data splits \n",
    "        train_data = total_data.iloc[train_index,:]\n",
    "        test_data = total_data.iloc[test_index,:]\n",
    "        \n",
    "        # transform the data\n",
    "        train_data.iloc[:,6:] = scaler.fit_transform(train_data.iloc[:,6:])\n",
    "        test_data.iloc[:,6:] = scaler.transform(test_data.iloc[:,6:])\n",
    "        \n",
    "        # Choose GMM\n",
    "        cluster_method = 'GMM'\n",
    "        \n",
    "        # organise training data, put in dictionary to pass to seed function \n",
    "        selected_columns = total_data.columns[6:].tolist()\n",
    "        train_data_GMM = train_data[selected_columns]\n",
    "        data_dict = {\"train_data\": train_data_GMM}\n",
    "        \n",
    "        # find median_fit model \n",
    "        median_model_GMM = clustering_with_seed(data_dict, n_clusters, cluster_method)\n",
    "        \n",
    "        # organise test data, get test set LL on  median_fit model\n",
    "        test_data_GMM = test_data[selected_columns]\n",
    "        fold_test_scores_GMM[fold_counter,cluster_counter] = median_model_GMM.score(test_data_GMM)\n",
    "        \n",
    "        # Choose HMM\n",
    "        cluster_method = 'HMM'\n",
    "        \n",
    "        # organise training sequences, put in dictionary to pass to seed function \n",
    "        train_sequences, train_sequence_lengths = HMM_Sequences(train_data)\n",
    "        data_dict = {\"train_sequences\": train_sequences,\n",
    "                     \"train_sequence_lengths\": train_sequence_lengths}\n",
    "        \n",
    "        # find median_fit model \n",
    "        median_model_HMM = clustering_with_seed(data_dict, n_clusters, cluster_method)\n",
    "        \n",
    "        # organise test sequences, get test set LL on  median_fit model\n",
    "        test_sequnces, test_sequence_lengths = HMM_Sequences(test_data)\n",
    "        fold_test_scores_HMM[fold_counter,cluster_counter] = median_model_HMM.score(test_sequnces, test_sequence_lengths)\n",
    "        \n",
    "        fold_counter +=1\n",
    "        \n",
    "    cluster_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83013ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, sharex=True)\n",
    "fig.suptitle(\"Activity - Average Log-Likelihood vs. Number of Cluster\")\n",
    "\n",
    "CFV_scores_GMM = np.mean(fold_test_scores_GMM,0) \n",
    "axs[0].plot(cluster_options, CFV_scores_GMM, marker = 'o', color='r', label='GMM')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_ylabel(\"Average Log-Likelihood\")\n",
    "axs[0].set_title(\"GMM\")\n",
    "\n",
    "CFV_scores_HMM = np.mean(fold_test_scores_HMM,0)\n",
    "axs[1].plot(cluster_options, CFV_scores_HMM, marker = 'o', color='g', label='HMM')\n",
    "axs[1].grid(True)\n",
    "axs[1].set_xticks(range(min(cluster_options),len(cluster_options)+min(cluster_options))) \n",
    "axs[1].set_xticklabels(cluster_options)\n",
    "axs[1].set_xlabel(\"Number of clusters\")\n",
    "axs[1].set_ylabel(\"Average Log-Likelihood\")\n",
    "axs[1].set_title(\"HMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046a84b-46d3-4c92-af4d-14ddc41bce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CFV_scores_GMM)\n",
    "print(CFV_scores_HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78b377-3c4e-480e-9439-13b50c29ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the data\n",
    "act_selection = {\n",
    "    \"Clusters\": cluster_options,\n",
    "    \"ACT_LL_GMM\": CFV_scores_GMM,\n",
    "    \"ACT_LL_HMM\": CFV_scores_HMM\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "plotdata = pd.DataFrame(act_selection)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "#plotdata.to_csv(\"xxx.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c879dfb",
   "metadata": {},
   "source": [
    "### Final Model Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6861c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "total_data_scaled = total_data.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "total_data_scaled.iloc[:,6:18] = scaler.fit_transform(total_data.iloc[:,6:18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d96ec-1abf-4e5d-a42d-21226f71e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_data_scaled.iloc[:,6:].columns)\n",
    "print(total_data_scaled.iloc[:, 6:18].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1943d637-80ee-42ad-8b40-bc27632dce25",
   "metadata": {},
   "source": [
    "#### Final GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969690f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose GMM and number of clusters \n",
    "cluster_method = 'GMM'\n",
    "optimal_num_clusters = 3\n",
    "        \n",
    "# Organise data, put in dictionary to pass to seed function \n",
    "selected_columns = total_data_scaled.iloc[:, 6:18].columns.tolist() ##### CHANGED BY ME\n",
    "total_data_scaled_GMM = total_data_scaled[selected_columns]\n",
    "data_dict = {\"train_data\": total_data_scaled_GMM}\n",
    "\n",
    "# find median_fit model \n",
    "final_model_GMM = clustering_with_seed(data_dict, optimal_num_clusters, cluster_method)\n",
    "\n",
    "# Predict cluster labels for the scaled dataset\n",
    "final_labels = final_model_GMM.predict(total_data_scaled_GMM)\n",
    "\n",
    "# Add the predicted labels as a new column in your dataframe\n",
    "total_data_scaled['gmm'] = final_labels\n",
    "\n",
    "# Calculate the number of unique participants\n",
    "num_unique_participants = total_data_scaled['p_id'].nunique()\n",
    "print(\"Number of unique participants:\", num_unique_participants)\n",
    "\n",
    "# Group by the predicted labels and calculate mean statistics\n",
    "state_profiles = total_data_scaled.groupby('gmm').agg({\n",
    "    'Sedentary_time_mean': 'mean', \n",
    "    'Light_activity_mean': 'mean',\n",
    "    'Moderate_activity_mean': 'mean', \n",
    "    'Vigorous_activity_mean': 'mean',\n",
    "    'Nighttime_activity_mean': 'mean',\n",
    "    'Total_Daily_Calories_mean': 'mean', \n",
    "    'Sedentary_time_sd': 'mean', \n",
    "    'Light_activity_sd': 'mean', \n",
    "    'Moderate_activity_sd': 'mean',\n",
    "    'Vigorous_activity_sd': 'mean', \n",
    "    'Nighttime_activity_sd': 'mean',\n",
    "    'Total_Daily_Calories_sd': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Add a column for the number of sequences in each cluster\n",
    "state_profiles['Num_Sequences'] = total_data_scaled.groupby('gmm').size().values\n",
    "display(state_profiles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478a20b-8ae0-4966-a415-f378746b24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_data_scaled.iloc[:, 0:18].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb644a2-9598-4be4-907e-cdbccb7cb106",
   "metadata": {},
   "source": [
    "#### Final HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd454df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose HMM\n",
    "cluster_method = 'HMM'\n",
    "optimal_num_clusters = 3\n",
    "\n",
    "total_sequences, total_sequence_lengths = HMM_Sequences(total_data_scaled.iloc[:, 0:18])\n",
    "data_dict = {\"train_sequences\": total_sequences,\n",
    "             \"train_sequence_lengths\": total_sequence_lengths}\n",
    "\n",
    "# find median_fit model \n",
    "final_model_HMM = clustering_with_seed(data_dict, optimal_num_clusters, cluster_method)\n",
    "\n",
    "# Predict the most likely state sequence for the entire dataset\n",
    "predicted_labels = final_model_HMM.predict(total_sequences, total_sequence_lengths)\n",
    "\n",
    "# Add predicted labels to your dataframe (if you want to attach them to the original data)\n",
    "total_data_scaled['hmm'] = predicted_labels\n",
    "\n",
    "# Calculate the number of unique participants\n",
    "num_unique_participants = total_data_scaled['p_id'].nunique()\n",
    "print(\"Number of unique participants:\", num_unique_participants)\n",
    "\n",
    "# Group by the predicted labels and calculate mean statistics\n",
    "state_profiles = total_data_scaled.groupby('hmm').agg({\n",
    "    'Sedentary_time_mean': 'mean', \n",
    "    'Light_activity_mean': 'mean',\n",
    "    'Moderate_activity_mean': 'mean', \n",
    "    'Vigorous_activity_mean': 'mean',\n",
    "    'Nighttime_activity_mean': 'mean',\n",
    "    'Total_Daily_Calories_mean': 'mean', \n",
    "    'Sedentary_time_sd': 'mean', \n",
    "    'Light_activity_sd': 'mean', \n",
    "    'Moderate_activity_sd': 'mean',\n",
    "    'Vigorous_activity_sd': 'mean', \n",
    "    'Nighttime_activity_sd': 'mean',\n",
    "    'Total_Daily_Calories_sd': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Add a column for the number of sequences in each cluster\n",
    "state_profiles['Num_Sequences'] = total_data_scaled.groupby('hmm').size().values\n",
    "display(state_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039dae4-7a9b-49f5-953d-d41b5bb47a23",
   "metadata": {},
   "source": [
    "#### HMM Transition Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87954e-a01f-4a6b-8a58-0529f29d7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the transition matrix\n",
    "transition_probabilities = final_model_HMM.transmat_\n",
    "rounded_transition_probabilities = np.round(transition_probabilities, 3)\n",
    "print(\"Rounded Transition Probabilities:\")\n",
    "print(rounded_transition_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6bf2d-c87c-4e9a-ae21-6a70b70161a7",
   "metadata": {},
   "source": [
    "#### HMM State Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e80b3-da3c-4019-93f5-eb5543264a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE DATA FOR PREDICTION ANALYSIS \n",
    "# Get the posterior probabilities for each state at each timepoint\n",
    "state_probabilities = final_model_HMM.predict_proba(total_sequences, total_sequence_lengths)\n",
    "\n",
    "state_prob_df = pd.DataFrame(state_probabilities, columns=[f'State_Prob_{i}' for i in range(optimal_num_clusters)])\n",
    "state_prob_df['timepoint'] = total_data_scaled['timepoint'].values\n",
    "state_prob_df['p_id'] = total_data_scaled['p_id'].values\n",
    "display(state_prob_df)\n",
    "\n",
    "total_data_scaled_probs = pd.merge(state_prob_df, total_data_scaled, on=['p_id', 'timepoint'], how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144d9f0-a57d-48e7-9f2b-a766d259b3b1",
   "metadata": {},
   "source": [
    "### Relabelling/Reordering the classes for GMM & HMM for Descriptives & prediction work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df172a0b-4378-48ae-947a-21768b004ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELABELING THE CLASSES: \n",
    "# Define the mapping (old value -> new value)\n",
    "hmm_mapping = {0: 2, \n",
    "               1: 3, \n",
    "               2: 1} \n",
    "gmm_mapping = {0: 3, \n",
    "               1: 2, \n",
    "               2: 1} \n",
    "\n",
    "# Apply the mapping to the 'Category' column\n",
    "total_data_scaled['hmm'] = total_data_scaled['hmm'].replace(hmm_mapping)\n",
    "total_data_scaled['gmm'] = total_data_scaled['gmm'].replace(gmm_mapping)\n",
    "\n",
    "# For the probs df\n",
    "total_data_scaled_probs['hmm'] = total_data_scaled_probs['hmm'].replace(hmm_mapping)\n",
    "total_data_scaled_probs['gmm'] = total_data_scaled_probs['gmm'].replace(gmm_mapping)\n",
    "\n",
    "# Rename columns using a dictionary\n",
    "total_data_scaled_probs.rename(columns={'State_Prob_0': 'State_Prob_2', \n",
    "                                        'State_Prob_1': 'State_Prob_3', \n",
    "                                        'State_Prob_2': 'State_Prob_1'}, inplace=True)\n",
    "\n",
    "# Print the DataFrame after renaming columns\n",
    "print(\"\\nDataFrame after renaming columns:\")\n",
    "print(total_data_scaled_probs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c0e1f-eaee-4ff0-8035-1ee571541dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frequency of values in the 'Category' column\n",
    "frequency = total_data_scaled_probs['hmm'].value_counts()\n",
    "print(frequency)\n",
    "\n",
    "frequency_gmm = total_data_scaled['gmm'].value_counts()\n",
    "print(frequency_gmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e734e9c",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35243bb-8cee-4a0c-8c16-38a8e053620b",
   "metadata": {},
   "source": [
    "### Demographic & Clinical Differences of the groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068186bd-c8c0-462a-9505-4da2bac937d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableone import TableOne\n",
    "import pandas as pd\n",
    "\n",
    "total_data_scaled_copy = total_data_scaled.copy()\n",
    "\n",
    "# Load the necessary data\n",
    "chara = pd.read_csv(\"chara_fvfvfv.csv\")\n",
    "age = pd.read_csv(\"age.csv\")\n",
    "comorbidities = pd.read_csv(\"Comorbities_Data_only.csv\")\n",
    "chara = chara[['p_id', 'timepoint', 'mh_family_TOTAL', 'gender_all', \n",
    "               'mh_family_depr___0', 'mh_family_depr___1', 'mh_family_depr___2',\n",
    "               'mh_longst_illness', 'LIFETIME_TRAUMA', 'WSAS_TOTAL', 'GAD7_TOTAL',\n",
    "               'IDS_TOTAL', 'IDS_CAT']]\n",
    "\n",
    "\n",
    "eth = pd.read_csv(\"Ethnicity_data.csv\")\n",
    "site = pd.read_csv(\"Site_data.csv\")  # 1 = London, 2=Spain, 3=Netherlands\n",
    "site['recruitmentsite'] = site['recruitmentsite'].astype('category')\n",
    "\n",
    "emp = pd.read_csv(\"employment_status.csv\")\n",
    "emp = emp[['p_id', 'EMPLOYMENT_STATUS']]\n",
    "emp['EMPLOYMENT_STATUS'] = emp['EMPLOYMENT_STATUS'].astype('category')\n",
    "\n",
    "# Remove duplicates to ensure unique participant IDs\n",
    "chara_desc_1p = chara.drop_duplicates(subset='p_id')\n",
    "age_desc_1p = age.drop_duplicates(subset='p_id')\n",
    "\n",
    "# Merge all relevant datasets\n",
    "merged_allPs = pd.merge(chara_desc_1p, site, on='p_id', how='left')\n",
    "merged_allPs = pd.merge(merged_allPs, eth, on='p_id', how='left')\n",
    "merged_allPs = pd.merge(merged_allPs, emp, on='p_id', how='left')\n",
    "merged_allPs = pd.merge(merged_allPs, comorbidities, on='p_id', how='left') \n",
    "merged_allPs = pd.merge(merged_allPs, age_desc_1p, on='p_id', how='left') \n",
    "\n",
    "# Select the columns you want to merge from merged_allPs\n",
    "columns_to_merge = ['p_id', 'hmm']\n",
    "merged_selected = total_data_scaled_copy[columns_to_merge]\n",
    "total_data_withcara = pd.merge(total_data_scaled_copy, merged_allPs, on='p_id', how='left')\n",
    "\n",
    "print(total_data_withcara.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308fdde-2c0d-45fb-a0d8-6ce051ebcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to include in the TableOne summary\n",
    "columns = ['age', 'gender_all_x', 'IDS_TOTAL_x',# 'IDS_CAT',\n",
    "           'recruitmentsite', 'mh_family_TOTAL', 'Mental_comorbidity',\n",
    "       'Physical_comorbidity','WSAS_TOTAL', 'GAD7_TOTAL', 'EMPLOYMENT_STATUS'] \n",
    "\n",
    "# Create the TableOne grouped by HMM\n",
    "table_activity_states = TableOne(total_data_withcara,                                       \n",
    "                                      columns=columns, \n",
    "                                      categorical=['gender_all_x', 'recruitmentsite', 'mh_family_TOTAL',\n",
    "                                                  'Mental_comorbidity',\n",
    "                                                   'Physical_comorbidity', 'EMPLOYMENT_STATUS'],\n",
    "                                      groupby='hmm',  # Group by 'hmm_mode'\n",
    "                                     # pval=True\n",
    "                                )\n",
    "                                      #normal_test=True)  \n",
    "\n",
    "# Display the grouped TableOne\n",
    "print(table_activity_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
